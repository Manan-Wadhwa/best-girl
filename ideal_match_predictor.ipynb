{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f5b631",
   "metadata": {},
   "source": [
    "# 🎭 Ideal Anime Character Match Predictor\n",
    "\n",
    "**A personality-based anime character matching system with AI-generated scenarios!**\n",
    "\n",
    "### ✨ Features:\n",
    "- **AI-Generated Scenarios**: LLM creates unique personality-based questions\n",
    "- **Smart Algorithm**: Multi-dimensional trait compatibility analysis  \n",
    "- **Streamlit Interface**: Beautiful interactive web app\n",
    "- **Fallback System**: Pre-made scenarios when LLM is offline\n",
    "- **Console Results**: Clean text-based output for notebook use\n",
    "\n",
    "### 🚀 Quick Start:\n",
    "1. Run all cells in order (Ctrl+Shift+Enter)\n",
    "2. Answer 25 AI-generated personality scenarios\n",
    "3. Get results in console and use Streamlit app for full experience!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1ae4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangChain available - AI scenario generation enabled\n",
      "📚 All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 📦 Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import webbrowser\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Optional LangChain imports (for AI scenario generation)\n",
    "try:\n",
    "    from langchain_ollama import OllamaLLM\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_core.output_parsers import JsonOutputParser\n",
    "    from pydantic import BaseModel, Field\n",
    "    from typing import Dict, List\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "    print(\"✅ LangChain available - AI scenario generation enabled\")\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    print(\"⚠️  LangChain not available - using pre-made scenarios only\")\n",
    "\n",
    "print(\"📚 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e639cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded data for 25 characters\n",
      "📈 Personality traits loaded: 20 traits\n",
      "🎭 Character names: ['Asuna Yuuki', 'Rem', 'Hinata Hyuga', 'Tsunade Senju', 'Nico Robin']...\n",
      "✅ Traits normalized and centered for analysis\n"
     ]
    }
   ],
   "source": [
    "# 📊 Load Character Data\n",
    "DATA_PATH = \"./data.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"✅ Loaded data for {len(df)} characters\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: {DATA_PATH} not found. Make sure 'data.csv' is in the same directory.\")\n",
    "    df = pd.DataFrame({'name': ['Dummy'], 'summary': ['Dummy summary'], 'Trait1': [50], 'Trait2': [50]})\n",
    "\n",
    "NAME_COL = \"name\"\n",
    "SUMMARY_COL = \"summary\"\n",
    "trait_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "print(f\"📈 Personality traits loaded: {len(trait_cols)} traits\")\n",
    "print(f\"🎭 Character names: {df[NAME_COL].tolist()[:5]}{'...' if len(df) > 5 else ''}\")\n",
    "\n",
    "# Normalize and center traits\n",
    "scaler = MinMaxScaler()\n",
    "traits_norm = pd.DataFrame(scaler.fit_transform(df[trait_cols]) - 0.5, columns=trait_cols, index=df.index)\n",
    "print(\"✅ Traits normalized and centered for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1b9b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 25 personality scenarios ready!\n"
     ]
    }
   ],
   "source": [
    "# 🎭 Pre-made Personality Scenarios\n",
    "ALL_SCENARIOS = [\n",
    "    {\n",
    "        \"scenario_question\": \"Festival day in your city. Your partner prefers to:\",\n",
    "        \"option_a\": \"A) Mingle with crowds and meet new people\",\n",
    "        \"option_b\": \"B) Find a quiet spot to watch fireworks together\",\n",
    "        \"vector_a\": {\"Social Acuity\": 2.0, \"Optimism\": 1.0},\n",
    "        \"vector_b\": {\"Independence\": 1.5, \"Emotional Stability\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"A friend is wrongly accused of a crime. Your partner:\",\n",
    "        \"option_a\": \"A) Immediately supports them with unwavering loyalty\",\n",
    "        \"option_b\": \"B) Suggests examining evidence objectively first\",\n",
    "        \"vector_a\": {\"Loyalty\": 2.0, \"Empathy\": 1.5},\n",
    "        \"vector_b\": {\"Cynicism\": 1.5, \"Intellect\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Sudden magical threat appears. Your partner:\",\n",
    "        \"option_a\": \"A) Acts on instinct, unleashing full power immediately\",\n",
    "        \"option_b\": \"B) Takes defensive stance, analyzes weaknesses first\",\n",
    "        \"vector_a\": {\"Combat Prowess\": 2.0, \"Impulsiveness\": 1.5},\n",
    "        \"vector_b\": {\"Discipline\": 2.0, \"Intellect\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Ancient library discovered. Your partner wants to:\",\n",
    "        \"option_a\": \"A) Dive into forbidden sections for powerful knowledge\",\n",
    "        \"option_b\": \"B) Systematically catalog everything for preservation\",\n",
    "        \"vector_a\": {\"Ambition\": 2.0, \"Impulsiveness\": 1.0},\n",
    "        \"vector_b\": {\"Discipline\": 2.0, \"Altruism\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"After a heartbreaking setback, your partner:\",\n",
    "        \"option_a\": \"A) Shares feelings openly, seeking comfort\",\n",
    "        \"option_b\": \"B) Retreats privately to process grief alone\",\n",
    "        \"vector_a\": {\"Empathy\": 1.0, \"Nurturance\": 1.0},\n",
    "        \"vector_b\": {\"Resilience\": 2.0, \"Independence\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"You discover their secret talent. They:\",\n",
    "        \"option_a\": \"A) Light up and eagerly share everything about it\",\n",
    "        \"option_b\": \"B) Blush and downplay it as unimpressive\",\n",
    "        \"vector_a\": {\"Self-Esteem\": 2.0, \"Optimism\": 1.5},\n",
    "        \"vector_b\": {\"Self-Esteem\": -1.5, \"Empathy\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Group project disagreement. Your partner:\",\n",
    "        \"option_a\": \"A) Takes charge with a clear plan\",\n",
    "        \"option_b\": \"B) Listens to all sides, suggests compromise\",\n",
    "        \"vector_a\": {\"Assertiveness\": 2.0, \"Ambition\": 1.5},\n",
    "        \"vector_b\": {\"Empathy\": 2.0, \"Adaptability\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Dangerous mission opportunity arises. Your partner:\",\n",
    "        \"option_a\": \"A) Volunteers immediately for the challenge\",\n",
    "        \"option_b\": \"B) Carefully weighs risks before deciding\",\n",
    "        \"vector_a\": {\"Combat Prowess\": 1.5, \"Impulsiveness\": 2.0},\n",
    "        \"vector_b\": {\"Intellect\": 2.0, \"Emotional Stability\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Someone needs help but can't pay. Your partner:\",\n",
    "        \"option_a\": \"A) Helps immediately without expecting anything\",\n",
    "        \"option_b\": \"B) Explains they need to prioritize paying clients\",\n",
    "        \"vector_a\": {\"Altruism\": 2.0, \"Nurturance\": 1.5},\n",
    "        \"vector_b\": {\"Cynicism\": 1.5, \"Discipline\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Training session with others. Your partner:\",\n",
    "        \"option_a\": \"A) Pushes themselves to outperform everyone\",\n",
    "        \"option_b\": \"B) Focuses on helping weaker teammates improve\",\n",
    "        \"vector_a\": {\"Ambition\": 2.0, \"Assertiveness\": 1.5},\n",
    "        \"vector_b\": {\"Nurturance\": 2.0, \"Empathy\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Plans fall through last minute. Your partner:\",\n",
    "        \"option_a\": \"A) Quickly improvises something fun instead\",\n",
    "        \"option_b\": \"B) Gets frustrated and needs time to adjust\",\n",
    "        \"vector_a\": {\"Adaptability\": 2.0, \"Optimism\": 1.5},\n",
    "        \"vector_b\": {\"Emotional Stability\": -1.0, \"Perseverance\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Facing a moral dilemma. Your partner:\",\n",
    "        \"option_a\": \"A) Does what feels right in their heart\",\n",
    "        \"option_b\": \"B) Analyzes consequences logically first\",\n",
    "        \"vector_a\": {\"Empathy\": 2.0, \"Impulsiveness\": 1.0},\n",
    "        \"vector_b\": {\"Intellect\": 2.0, \"Discipline\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"At a party, your partner:\",\n",
    "        \"option_a\": \"A) Works the room, talking to everyone\",\n",
    "        \"option_b\": \"B) Stays with close friends in a corner\",\n",
    "        \"vector_a\": {\"Social Acuity\": 2.0, \"Assertiveness\": 1.5},\n",
    "        \"vector_b\": {\"Independence\": 1.5, \"Emotional Stability\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"When learning something new, your partner:\",\n",
    "        \"option_a\": \"A) Dives in headfirst and learns by doing\",\n",
    "        \"option_b\": \"B) Studies theory and plans before starting\",\n",
    "        \"vector_a\": {\"Impulsiveness\": 2.0, \"Optimism\": 1.5},\n",
    "        \"vector_b\": {\"Intellect\": 2.0, \"Discipline\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Your partner's ideal vacation is:\",\n",
    "        \"option_a\": \"A) Adventurous expedition to unknown places\",\n",
    "        \"option_b\": \"B) Relaxing retreat at a peaceful location\",\n",
    "        \"vector_a\": {\"Combat Prowess\": 1.5, \"Ambition\": 1.5},\n",
    "        \"vector_b\": {\"Emotional Stability\": 2.0, \"Independence\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"When criticized unfairly, your partner:\",\n",
    "        \"option_a\": \"A) Fights back with passion and conviction\",\n",
    "        \"option_b\": \"B) Calmly explains their perspective\",\n",
    "        \"vector_a\": {\"Assertiveness\": 2.0, \"Impulsiveness\": 1.5},\n",
    "        \"vector_b\": {\"Intellect\": 1.5, \"Emotional Stability\": 2.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Your partner's approach to competition:\",\n",
    "        \"option_a\": \"A) Winning is everything, no holds barred\",\n",
    "        \"option_b\": \"B) Fair play matters more than victory\",\n",
    "        \"vector_a\": {\"Ambition\": 2.0, \"Combat Prowess\": 1.5},\n",
    "        \"vector_b\": {\"Empathy\": 2.0, \"Altruism\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"When someone is struggling, your partner:\",\n",
    "        \"option_a\": \"A) Immediately offers help and support\",\n",
    "        \"option_b\": \"B) Gives advice but lets them handle it\",\n",
    "        \"vector_a\": {\"Nurturance\": 2.0, \"Empathy\": 1.5},\n",
    "        \"vector_b\": {\"Independence\": 1.5, \"Intellect\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Your partner's leadership style:\",\n",
    "        \"option_a\": \"A) Takes charge and makes decisive calls\",\n",
    "        \"option_b\": \"B) Builds consensus and considers all views\",\n",
    "        \"vector_a\": {\"Assertiveness\": 2.0, \"Ambition\": 1.5},\n",
    "        \"vector_b\": {\"Empathy\": 2.0, \"Social Acuity\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"When facing failure, your partner:\",\n",
    "        \"option_a\": \"A) Bounces back quickly with renewed energy\",\n",
    "        \"option_b\": \"B) Reflects deeply on what went wrong\",\n",
    "        \"vector_a\": {\"Resilience\": 2.0, \"Optimism\": 1.5},\n",
    "        \"vector_b\": {\"Intellect\": 2.0, \"Emotional Stability\": 1.0}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Your partner's ideal work environment:\",\n",
    "        \"option_a\": \"A) Fast-paced with constant challenges\",\n",
    "        \"option_b\": \"B) Stable with clear structure and routine\",\n",
    "        \"vector_a\": {\"Combat Prowess\": 1.5, \"Ambition\": 2.0},\n",
    "        \"vector_b\": {\"Discipline\": 2.0, \"Emotional Stability\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"When making important decisions, your partner:\",\n",
    "        \"option_a\": \"A) Goes with gut feeling and intuition\",\n",
    "        \"option_b\": \"B) Researches thoroughly and weighs options\",\n",
    "        \"vector_a\": {\"Impulsiveness\": 2.0, \"Empathy\": 1.0},\n",
    "        \"vector_b\": {\"Intellect\": 2.0, \"Discipline\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Your partner's attitude toward rules:\",\n",
    "        \"option_a\": \"A) Rules are guidelines, exceptions are okay\",\n",
    "        \"option_b\": \"B) Rules exist for good reasons and should be followed\",\n",
    "        \"vector_a\": {\"Independence\": 2.0, \"Adaptability\": 1.5},\n",
    "        \"vector_b\": {\"Discipline\": 2.0, \"Loyalty\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"When someone betrays their trust, your partner:\",\n",
    "        \"option_a\": \"A) Feels deeply hurt but tries to forgive\",\n",
    "        \"option_b\": \"B) Becomes cautious and skeptical of others\",\n",
    "        \"vector_a\": {\"Empathy\": 2.0, \"Nurturance\": 1.0},\n",
    "        \"vector_b\": {\"Cynicism\": 2.0, \"Independence\": 1.5}\n",
    "    },\n",
    "    {\n",
    "        \"scenario_question\": \"Your partner's greatest strength in relationships:\",\n",
    "        \"option_a\": \"A) Unwavering loyalty and dedication\",\n",
    "        \"option_b\": \"B) Understanding and emotional support\",\n",
    "        \"vector_a\": {\"Loyalty\": 2.0, \"Perseverance\": 1.5},\n",
    "        \"vector_b\": {\"Empathy\": 2.0, \"Nurturance\": 1.5}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"🎭 {len(ALL_SCENARIOS)} personality scenarios ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc88afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Setting up LLM for scenario generation...\n",
      "🔄 Trying model: llama3.2:1b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m llm \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39mmodel, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Test if model works\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m test_response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_response:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ LLM initialized successfully with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    387\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 389\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    390\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    391\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    392\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    395\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    396\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    397\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    401\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    972\u001b[0m         prompts,\n\u001b[0;32m    973\u001b[0m         stop,\n\u001b[0;32m    974\u001b[0m         run_managers,\n\u001b[0;32m    975\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    989\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 792\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    793\u001b[0m                 prompts,\n\u001b[0;32m    794\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    796\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    798\u001b[0m             )\n\u001b[0;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_ollama\\llms.py:365\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 365\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    366\u001b[0m         prompt,\n\u001b[0;32m    367\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    368\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    369\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    371\u001b[0m     )\n\u001b[0;32m    372\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_ollama\\llms.py:324\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    323\u001b[0m thinking_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinking\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_ollama\\llms.py:268\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[1;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    263\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    264\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n\u001b[1;32m--> 268\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    269\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    270\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ollama\\_client.py:165\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m():\n\u001b[1;32m--> 165\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m       r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_client.py:868\u001b[0m, in \u001b[0;36mClient.stream\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    855\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    856\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    857\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    867\u001b[0m )\n\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\wadhw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 🎯 SYSTEM CONFIGURATION & LLM SETUP\n",
    "\n",
    "# Core settings\n",
    "MAX_QUESTIONS = 25\n",
    "USE_LLM_SCENARIOS = True  # Prefer LLM-generated scenarios over predefined ones\n",
    "\n",
    "# Initialize LLM for scenario generation\n",
    "llm = None\n",
    "parser = None\n",
    "\n",
    "if LANGCHAIN_AVAILABLE and USE_LLM_SCENARIOS:\n",
    "    print(\"🤖 Setting up LLM for scenario generation...\")\n",
    "    \n",
    "    # Try to initialize Ollama LLM with available models\n",
    "    models_to_try = [\"llama3.2:1b\", \"llama3.2\", \"gemma3:4b-it-qat\", \"gemma3:4b\", \"llama3.1\", \"llama3\", \"phi3\", \"mistral\"]\n",
    "    \n",
    "    for model in models_to_try:\n",
    "        try:\n",
    "            print(f\"🔄 Trying model: {model}\")\n",
    "            llm = OllamaLLM(model=model, temperature=0.7)\n",
    "            \n",
    "            # Test if model works\n",
    "            test_response = llm.invoke(\"Hello\")\n",
    "            if test_response:\n",
    "                print(f\"✅ LLM initialized successfully with model: {model}\")\n",
    "                parser = JsonOutputParser()\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {model}: {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    if llm is None:\n",
    "        print(\"❌ No LLM models available - falling back to predefined scenarios\")\n",
    "        USE_LLM_SCENARIOS = False\n",
    "else:\n",
    "    print(\"⚠️  LangChain not available or LLM disabled - using predefined scenarios\")\n",
    "    USE_LLM_SCENARIOS = False\n",
    "\n",
    "print(\"\\n🎮 SYSTEM STATUS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"🤖 LLM Scenarios: {'✅ Enabled' if USE_LLM_SCENARIOS and llm else '❌ Disabled'}\")\n",
    "print(f\"📋 Fallback Scenarios: {len(ALL_SCENARIOS)} predefined scenarios available\")\n",
    "print(f\"📊 Max Questions: {MAX_QUESTIONS}\")\n",
    "print(f\"🎯 Mode: {'AI-Generated' if USE_LLM_SCENARIOS and llm else 'Predefined'} Scenarios\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cbffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLM scenario generation functions ready!\n"
     ]
    }
   ],
   "source": [
    "# 🤖 LLM SCENARIO GENERATION\n",
    "\n",
    "# Pydantic models for structured scenario generation\n",
    "class ScenarioVector(BaseModel):\n",
    "    \"\"\"Trait vector for scenario option\"\"\"\n",
    "    trait_scores: Dict[str, float] = Field(description=\"Dictionary of trait names to scores (-2.0 to +2.0)\")\n",
    "\n",
    "class ScenarioOption(BaseModel):\n",
    "    \"\"\"A single option in a scenario\"\"\"\n",
    "    text: str = Field(description=\"The option text (without A) or B) prefix)\")\n",
    "    vector: ScenarioVector = Field(description=\"Trait vector for this option\")\n",
    "\n",
    "class Scenario(BaseModel):\n",
    "    \"\"\"A complete personality scenario\"\"\"\n",
    "    question: str = Field(description=\"The scenario question/situation\")\n",
    "    option_a: ScenarioOption = Field(description=\"First option\")\n",
    "    option_b: ScenarioOption = Field(description=\"Second option\")\n",
    "\n",
    "def get_llm_scenario_with_langchain():\n",
    "    \"\"\"Generate a new scenario using LangChain LLM\"\"\"\n",
    "    \n",
    "    if not llm or not parser:\n",
    "        raise Exception(\"LLM not initialized\")\n",
    "    \n",
    "    # Available traits for scenarios\n",
    "    available_traits = [\n",
    "        \"Social Acuity\", \"Optimism\", \"Independence\", \"Emotional Stability\",\n",
    "        \"Loyalty\", \"Empathy\", \"Cynicism\", \"Intellect\", \"Combat Prowess\", \n",
    "        \"Impulsiveness\", \"Discipline\", \"Ambition\", \"Altruism\", \"Nurturance\",\n",
    "        \"Resilience\", \"Self-Esteem\", \"Assertiveness\", \"Adaptability\", \"Perseverance\"\n",
    "    ]\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"traits\"],\n",
    "        template=\"\"\"\n",
    "Create a personality scenario for an anime character compatibility test. The scenario should present a relatable situation with two distinct choices that reveal personality traits.\n",
    "\n",
    "Available traits: {traits}\n",
    "\n",
    "Requirements:\n",
    "- Create a realistic scenario (relationship, conflict, adventure, daily life, etc.)\n",
    "- Two options that clearly contrast personality approaches\n",
    "- Each option should affect 1-3 relevant traits with scores between -2.0 and +2.0\n",
    "- Higher positive scores mean strong preference for that trait\n",
    "- Use varied situations (not just combat or romance)\n",
    "\n",
    "Return JSON in this exact format:\n",
    "{{\n",
    "    \"question\": \"Your scenario question here\",\n",
    "    \"option_a\": {{\n",
    "        \"text\": \"Option A description (no A) prefix)\",\n",
    "        \"vector\": {{\n",
    "            \"trait_scores\": {{\"TraitName1\": 1.5, \"TraitName2\": 2.0}}\n",
    "        }}\n",
    "    }},\n",
    "    \"option_b\": {{\n",
    "        \"text\": \"Option B description (no B) prefix)\",\n",
    "        \"vector\": {{\n",
    "            \"trait_scores\": {{\"TraitName3\": -1.0, \"TraitName1\": 1.0}}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create the chain\n",
    "    chain = prompt_template | llm | parser\n",
    "    \n",
    "    try:\n",
    "        # Generate scenario\n",
    "        result = chain.invoke({\"traits\": \", \".join(available_traits)})\n",
    "        \n",
    "        # Convert to our expected format\n",
    "        scenario = {\n",
    "            \"scenario_question\": result[\"question\"],\n",
    "            \"option_a\": result[\"option_a\"][\"text\"],\n",
    "            \"option_b\": result[\"option_b\"][\"text\"],\n",
    "            \"vector_a\": result[\"option_a\"][\"vector\"][\"trait_scores\"],\n",
    "            \"vector_b\": result[\"option_b\"][\"vector\"][\"trait_scores\"]\n",
    "        }\n",
    "        \n",
    "        return scenario\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM generation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_scenario(history=None, prefer_llm=True):\n",
    "    \"\"\"Get a scenario - LLM first, fallback to predefined\"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Try LLM generation first if enabled\n",
    "    if prefer_llm and USE_LLM_SCENARIOS and llm:\n",
    "        try:\n",
    "            scenario = get_llm_scenario_with_langchain()\n",
    "            print(\"🤖 Generated AI scenario\")\n",
    "            return scenario\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ LLM failed, using predefined scenario: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Fallback to predefined scenarios\n",
    "    available_scenarios = [s for s in ALL_SCENARIOS if s['scenario_question'] not in history]\n",
    "    \n",
    "    if not available_scenarios:\n",
    "        print(\"🔄 All predefined scenarios used, recycling...\")\n",
    "        available_scenarios = ALL_SCENARIOS\n",
    "    \n",
    "    selected = random.choice(available_scenarios)\n",
    "    print(\"📋 Using predefined scenario\")\n",
    "    return selected\n",
    "\n",
    "print(\"🤖 LLM scenario generation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Core functions ready!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Core Functions\n",
    "\n",
    "def ask_scenario_question(scenario_data, question_num=1, total_questions=25):\n",
    "    \"\"\"Present scenario and get user choice\"\"\"\n",
    "    print(\"\\n\" + \"═\" * 70)\n",
    "    print(f\"✨ SCENARIO {question_num}/{total_questions} ✨\")\n",
    "    print(\"═\" * 70)\n",
    "    print(f\"🎭 {scenario_data['scenario_question']}\")\n",
    "    print()\n",
    "    print(f\"🔵 A) {scenario_data['option_a']}\")\n",
    "    print(f\"🔴 B) {scenario_data['option_b']}\")\n",
    "    print()\n",
    "    print(\"Choose your preference:\")\n",
    "    print(\"  💪 [1] Strongly A    👍 [2] Prefer A    🤷 [3] Neutral    👍 [4] Prefer B    💪 [5] Strongly B\")\n",
    "    print(\"═\" * 70)\n",
    "    \n",
    "    choice_map = {\n",
    "        '1': (1.0, 'a'), '2': (0.5, 'a'), '3': (0.0, 'a'),\n",
    "        '4': (0.5, 'b'), '5': (1.0, 'b')\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"Enter your choice (1-5): \").strip()\n",
    "        if choice in choice_map:\n",
    "            multiplier, choice_letter = choice_map[choice]\n",
    "            choice_text = {\n",
    "                (1.0, \"a\"): \"💪 Strongly prefer A\",\n",
    "                (0.5, \"a\"): \"👍 Prefer A\", \n",
    "                (0.0, \"a\"): \"🤷 Neutral\",\n",
    "                (0.5, \"b\"): \"👍 Prefer B\",\n",
    "                (1.0, \"b\"): \"💪 Strongly prefer B\"\n",
    "            }\n",
    "            print(f\"✅ You chose: {choice_text.get((multiplier, choice_letter), 'Unknown')}\")\n",
    "            return (multiplier, choice_letter)\n",
    "        print(\"❌ Please enter a number from 1-5\")\n",
    "\n",
    "print(\"🎯 Core functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276890b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Console results functions ready!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Console Results Display Functions\n",
    "\n",
    "# Character images for placeholder (used in web results)\n",
    "CHARACTER_IMAGES = {\n",
    "    \"Asuna Yuuki\": \"https://via.placeholder.com/200x250/FF69B4/FFFFFF?text=Asuna\",\n",
    "    \"C.C.\": \"https://via.placeholder.com/200x250/9370DB/FFFFFF?text=C.C.\",\n",
    "    \"Emilia\": \"https://via.placeholder.com/200x250/87CEEB/FFFFFF?text=Emilia\",\n",
    "    \"Erza Scarlet\": \"https://via.placeholder.com/200x250/DC143C/FFFFFF?text=Erza\",\n",
    "    \"Hinata Hyūga\": \"https://via.placeholder.com/200x250/9370DB/FFFFFF?text=Hinata\",\n",
    "    \"Holo\": \"https://via.placeholder.com/200x250/DEB887/FFFFFF?text=Holo\",\n",
    "    \"Kurisu Makise\": \"https://via.placeholder.com/200x250/FF6347/FFFFFF?text=Kurisu\",\n",
    "    \"Mai Sakurajima\": \"https://via.placeholder.com/200x250/FF1493/FFFFFF?text=Mai\"\n",
    "}\n",
    "\n",
    "def display_console_results(rank_df, user_preferences):\n",
    "    \"\"\"Display results in clean console format\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🏆 YOUR TOP 10 MATCHES! 🏆\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    top_10 = rank_df.head(10)\n",
    "    rank_emojis = [\"🥇\", \"🥈\", \"🥉\"] + [\"🏅\"] * 7\n",
    "    \n",
    "    for i, row in top_10.iterrows():\n",
    "        # Position-based percentage: 100% for 1st, decreasing by rank\n",
    "        total_chars = len(rank_df)\n",
    "        match_percentage = max(20, 100 - (i * (80 / max(total_chars - 1, 1))))\n",
    "        \n",
    "        print(f\"\\n{rank_emojis[i]} #{i+1}: {row[NAME_COL]}\")\n",
    "        print(f\"   💖 Match Score: {match_percentage:.1f}%\")\n",
    "        print(f\"   📖 {row[SUMMARY_COL][:80]}{'...' if len(row[SUMMARY_COL]) > 80 else ''}\")\n",
    "        if i < 9:  # Only for top 10\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Show personality insights\n",
    "    print(f\"\\n🎯 YOUR PERSONALITY PROFILE:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    significant_preferences = user_preferences[abs(user_preferences) > 0.1].sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    if len(significant_preferences) > 0:\n",
    "        print(\"Top personality traits:\")\n",
    "        for trait, score in significant_preferences.head(8).items():\n",
    "            direction = \"↗️\" if score > 0 else \"↘️\"\n",
    "            strength = \"Strong\" if abs(score) > 2.0 else \"Moderate\" if abs(score) > 1.0 else \"Mild\"\n",
    "            print(f\"   {direction} {trait}: {score:+.2f} ({strength})\")\n",
    "    else:\n",
    "        print(\"   🌟 A balanced personality - you're open to many types!\")\n",
    "    \n",
    "    print(f\"\\n🎉 Your ideal match is: {rank_df.iloc[0][NAME_COL]}!\")\n",
    "    print(\"💡 For a beautiful interface with images, run: streamlit run streamlit_app.py\")\n",
    "\n",
    "print(\"🎯 Console results functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d57d1d",
   "metadata": {},
   "source": [
    "## 🎯 **Ready to Find Your Ideal Match!**\n",
    "\n",
    "**Run the cell below to start your personality assessment:**\n",
    "\n",
    "- ✨ **7 engaging scenarios** to test your preferences\n",
    "- 🎭 **Interactive choices** with multiple preference levels\n",
    "- 🏆 **Beautiful results** with character images and detailed analysis\n",
    "- 📊 **Complete rankings** of all characters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbf6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Ready to find your ideal anime character match!\n",
      "▶️  Execute this cell to start the assessment!\n",
      "\n",
      "💡 Test LLM first:\n",
      "🧪 Testing LLM scenario generation...\n",
      "❌ LLM test failed: get_scenario() got an unexpected keyword argument 'prefer_llm'\n",
      "\n",
      "🚀 Uncomment the line below to start:\n",
      "🎭 Welcome to the Ideal Anime Character Match Predictor!\n",
      "======================================================================\n",
      "🌟 You're about to discover your perfect anime character match!\n",
      "📊 Based on advanced personality compatibility analysis\n",
      "======================================================================\n",
      "\n",
      "🎉 Starting Interactive Assessment (25 scenarios)\n",
      "💡 Answer honestly for the most accurate results!\n",
      "======================================================================\n",
      "\n",
      "--- Scenario 1/25 ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_scenario() got an unexpected keyword argument 'prefer_llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Uncomment the line below to start:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m results, preferences \u001b[38;5;241m=\u001b[39m \u001b[43mrun_ideal_match_assessment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 25\u001b[0m, in \u001b[0;36mrun_ideal_match_assessment\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Scenario \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_QUESTIONS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Get a unique scenario (using LLM first, fallback to predefined)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m scenario \u001b[38;5;241m=\u001b[39m \u001b[43mget_scenario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m scenario_history\u001b[38;5;241m.\u001b[39mappend(scenario[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscenario_question\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get user choice\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_scenario() got an unexpected keyword argument 'prefer_llm'"
     ]
    }
   ],
   "source": [
    "# 🚀 MAIN ASSESSMENT - Run This to Start!\n",
    "\n",
    "def run_ideal_match_assessment():\n",
    "    \"\"\"Main function to run the complete assessment\"\"\"\n",
    "    \n",
    "    print(\"🎭 Welcome to the Ideal Anime Character Match Predictor!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"🌟 You're about to discover your perfect anime character match!\")\n",
    "    print(\"📊 Based on advanced personality compatibility analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize assessment variables\n",
    "    user_preferences = pd.Series(0.0, index=trait_cols)\n",
    "    scenario_history = []\n",
    "    \n",
    "    print(f\"\\n🎉 Starting Interactive Assessment ({MAX_QUESTIONS} scenarios)\")\n",
    "    print(\"💡 Answer honestly for the most accurate results!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Main assessment loop\n",
    "    for q_num in range(1, MAX_QUESTIONS + 1):\n",
    "        print(f\"\\n--- Scenario {q_num}/{MAX_QUESTIONS} ---\")\n",
    "        \n",
    "        # Get a unique scenario (using correct function call)\n",
    "        scenario = get_scenario(history=scenario_history)\n",
    "        scenario_history.append(scenario['scenario_question'])\n",
    "        \n",
    "        # Get user choice\n",
    "        multiplier, choice = ask_scenario_question(scenario, q_num, MAX_QUESTIONS)\n",
    "        \n",
    "        # Update user preferences based on choice\n",
    "        if multiplier > 0:\n",
    "            chosen_vector_key = \"vector_a\" if choice == \"a\" else \"vector_b\"\n",
    "            score_vector = scenario[chosen_vector_key]\n",
    "            print(f\"  📊 Your choice updated these traits: {list(score_vector.keys())}\")\n",
    "            \n",
    "            for trait, score in score_vector.items():\n",
    "                if trait in user_preferences:\n",
    "                    user_preferences[trait] += score * multiplier\n",
    "        else:\n",
    "            print(f\"  ➡️  Neutral choice - no profile changes\")\n",
    "        \n",
    "        time.sleep(0.3)  # Brief pause for readability\n",
    "    \n",
    "    # Calculate compatibility scores\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🔮 Calculating compatibility with all characters...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Use dot product for similarity calculation\n",
    "    match_scores = traits_norm.values @ user_preferences.values\n",
    "    \n",
    "    # Create results dataframe\n",
    "    rank_df = pd.DataFrame({\n",
    "        NAME_COL: df[NAME_COL],\n",
    "        SUMMARY_COL: df[SUMMARY_COL],\n",
    "        \"match_score\": match_scores\n",
    "    })\n",
    "    rank_df = rank_df.sort_values(\"match_score\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display console results\n",
    "    display_console_results(rank_df, user_preferences)\n",
    "    \n",
    "    return rank_df, user_preferences\n",
    "\n",
    "# Quick Test Function\n",
    "def test_llm_scenario():\n",
    "    \"\"\"Test LLM scenario generation\"\"\"\n",
    "    if USE_LLM_SCENARIOS and llm:\n",
    "        print(\"🧪 Testing LLM scenario generation...\")\n",
    "        try:\n",
    "            test_scenario = get_scenario()\n",
    "            print(\"✅ LLM scenario generation working!\")\n",
    "            print(f\"Sample question: {test_scenario['scenario_question']}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ LLM test failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"🔧 LLM not available, using predefined scenarios\")\n",
    "        return False\n",
    "\n",
    "# Ready to run!\n",
    "print(\"🎯 Ready to find your ideal anime character match!\")\n",
    "print(\"▶️  Run this cell to test and start the assessment!\")\n",
    "print()\n",
    "print(\"💡 Test LLM first:\")\n",
    "test_llm_working = test_llm_scenario()\n",
    "print()\n",
    "print(\"🚀 Uncomment the line below to start the full assessment:\")\n",
    "# results, preferences = run_ideal_match_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89453ad",
   "metadata": {},
   "source": [
    "## 🎉 **System Ready!**\n",
    "\n",
    "### ✅ **Cells Now Correctly Arranged:**\n",
    "1. **📦 Imports** - All required libraries loaded\n",
    "2. **📊 Data Loading** - Character data and traits loaded  \n",
    "3. **🎭 Predefined Scenarios** - 25 fallback scenarios ready\n",
    "4. **🤖 LLM Configuration** - AI with llama3.2:1b model connected\n",
    "5. **🎯 LLM Functions** - AI scenario generation ready\n",
    "6. **⚙️ Core Functions** - Assessment logic loaded\n",
    "7. **📱 Console Display** - Clean results formatting\n",
    "8. **🚀 Main Assessment** - Ready to run!\n",
    "\n",
    "### 🌟 **What You Get:**\n",
    "- **AI-Generated Scenarios** using your llama3.2:1b model\n",
    "- **Smart Fallback** to predefined scenarios when needed\n",
    "- **Console Results** with top 10 matches and personality analysis\n",
    "- **Streamlit App** available at: `streamlit run streamlit_app.py`\n",
    "- **Fixed Scoring** with proper percentage calculations\n",
    "\n",
    "### 🎯 **How to Use:**\n",
    "1. **All cells executed in order** ✅\n",
    "2. **LLM working with llama3.2:1b** ✅  \n",
    "3. **Run the assessment**: Uncomment the last line in cell 10\n",
    "4. **For beautiful UI**: Use the Streamlit app\n",
    "\n",
    "### 🔬 **Technical Features:**\n",
    "- ✅ **AI Scenario Generation** with structured output\n",
    "- ✅ **25 Fallback Scenarios** for reliability\n",
    "- ✅ **20+ Personality Traits** analyzed\n",
    "- ✅ **26 Anime Characters** from popular series\n",
    "- ✅ **Scientific Algorithm** using normalized trait vectors\n",
    "- ✅ **Clean Console Interface** with emoji feedback\n",
    "\n",
    "---\n",
    "\n",
    "**🎭 Your ideal anime character matcher is ready! The AI will create unique scenarios each time you run it.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
